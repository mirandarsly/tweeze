# -*- coding: utf-8 -*-
"""Tweeze_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yVEcufDfTb1lIoCqAMcCUyWHWirX9tyU

# Input Data
"""

from google.colab import drive
drive.mount('/content/Dataset_Destinasi')

import pandas as pd

# Read tempat.csv
tempat_url = "https://drive.google.com/file/d/1Al31EMPoXSV_BP2AN0Y0LHT3UJrL-yOH/view?usp=sharing"
tempat_file_id = tempat_url.split("/")[-2]
tempat_download_link = f"https://drive.google.com/uc?id={tempat_file_id}"
tempat = pd.read_csv(tempat_download_link)

# Read rating.csv
rating_url = "https://drive.google.com/file/d/1APCMg75sUZ9JdBR9g1YS07jCpQ-U9zyk/view?usp=sharing"
rating_file_id = rating_url.split("/")[-2]
rating_download_link = f"https://drive.google.com/uc?id={rating_file_id}"
rating = pd.read_csv(rating_download_link)

# Read user.csv
user_url = "https://drive.google.com/file/d/1ecsZzNL0gQzQV9EguyUy2Gp-qXGAJnm5/view?usp=sharing"
user_file_id = user_url.split("/")[-2]
user_download_link = f"https://drive.google.com/uc?id={user_file_id}"
user = pd.read_csv(user_download_link)

tempat.sample(5)

rating.head(5)

user.sample(5)

print(f"Number of places in the dataset: {len(tempat['Place_Id'].unique())}")
print(f"Number of users: {len(user['User_Id'].unique())}")
print(f"The number of ratings given by the user to the dataset: {len(rating['User_Id'])}")

"""# Exploratory Data Analysis"""

tempat.info()

tempat.isnull().sum()

rating.info()

rating.isnull().sum()

user.info()

user.isnull().sum()

tempat.Category.unique()

"""# Data Preprocessing"""

#from sklearn.preprocessing import MinMaxScaler

# Data preprocessing
#scaler = MinMaxScaler()
#x_train_scaled = scaler.fit_transform(x_train)
#x_val_scaled = scaler.transform(x_val)

import numpy as np

tourism_all = np.concatenate((
    tempat.Place_Id.unique(),
    rating.Place_Id.unique()
))

tourism_all = np.sort(np.unique(tourism_all))

print(f"Total number of tourism: {len(tourism_all)}")

gabungan_rate = rating
gabungan_rate

gabungan = pd.merge(gabungan_rate,tempat[["Place_Id","Place_Name","Description","City","Category"]],on='Place_Id', how='left')
gabungan

gabungan['city_category'] = gabungan[['City','Category']].agg(' '.join,axis=1)

gabungan

"""# Data Preparation

## Missing Values
"""

gabungan.isnull().sum()

preparation= gabungan.drop_duplicates("Place_Id")
preparation

place_id = preparation.Place_Id.tolist()

place_name = preparation.Place_Name.tolist()

place_category = preparation.Category.tolist()

place_desc = preparation.Description.tolist()

place_city = preparation.City.tolist()

city_category = preparation.city_category.tolist()

tourism_new = pd.DataFrame({
    "id":place_id,
    "name":place_name,
    "category":place_category,
    "description":place_desc,
    "city":place_city,
    "city_category":city_category
})

tourism_new

"""# Colaborative Filtering """

import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

df = rating
df

"""### Encode"""

user_ids = df.User_Id.unique().tolist()

user_to_user_encoded = {x:i for i, x in enumerate(user_ids)}

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

place_ids = df.Place_Id.unique().tolist()

place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}

place_encoded_to_place = {x: i for x, i in enumerate(place_ids)}

df['user'] = df.User_Id.map(user_to_user_encoded)

df['place'] = df.Place_Id.map(place_to_place_encoded)

num_users = len(user_to_user_encoded)

num_place = len(place_encoded_to_place)

df['Place_Ratings'] = df['Place_Ratings'].values.astype(np.float32)

min_rating = min(df['Place_Ratings'])

max_rating= max(df['Place_Ratings'])

print('Number of User: {}, Number of Place: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_place, min_rating, max_rating
))

"""## Train Test Split"""

df = df.sample(frac=1,random_state=36)
df

x = df[['user','place']].values

y = df['Place_Ratings'].apply(lambda x:(x-min_rating)/(max_rating-min_rating)).values

train_indices = int(0.8 * df.shape[0])

x_train,x_val,y_train,y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices].reshape((-1,1)), #reshape y_train
    y[train_indices:].reshape((-1,1))  #reshape y_val
)

print(x,y)

"""## Training"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_place, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_place = num_place
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.place_embedding = layers.Embedding(
            num_place,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.place_bias = layers.Embedding(num_place, 1)
        self.dense_layer = layers.Dense(128, activation='relu')  # Add an additional dense layer
        self.output_layer = layers.Dense(1, activation='sigmoid')  # Add output layer with 1 unit

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        place_vector = self.place_embedding(inputs[:, 1])
        place_bias = self.place_bias(inputs[:, 1])

        dot_user_place = tf.tensordot(user_vector, place_vector, 2)

        x = dot_user_place + user_bias + place_bias
        x = self.dense_layer(x)  # Apply the additional dense layer
        x = self.output_layer(x)  # Apply output layer with 1 unit and sigmoid activation
        return x

model = RecommenderNet(num_users, num_place, 100)

model.compile(
    loss=tf.keras.losses.MeanSquaredError(),
    optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # Update learning rate
    metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val),
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

place_df = tourism_new
df = rating

user_id = df.User_Id.sample(1).iloc[0]
place_visited_by_user = df[df.User_Id == user_id]

place_not_visited = place_df[~place_df['id'].isin(place_visited_by_user['Place_Id'].values)]['id'] 
place_not_visited = list(
    set(place_not_visited)
    .intersection(set(place_to_place_encoded.keys()))
)
 
place_not_visited = [[place_to_place_encoded.get(x)] for x in place_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_place_array = np.hstack(
    ([[user_encoder]] * len(place_not_visited), place_not_visited)
)

ratings = model.predict(user_place_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_place_ids = [
    place_encoded_to_place.get(place_not_visited[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Place with high ratings from user')
print('----' * 8)
 
top_place_user = (
    place_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)
 
place_df_rows = place_df[place_df['id'].isin(top_place_user)]
pd.DataFrame(place_df_rows)

print('----' * 8)
print('Top 10 place recommendation')
print('----' * 8)
 
recommended_place = place_df[place_df['id'].isin(recommended_place_ids)]
recommended_place