# -*- coding: utf-8 -*-
"""Tweeze Collaborative Filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OX_afcEu0oEaXFI-zSviZ-6fgAYDqNi0

# Importing Data
"""

import pandas as pd
import numpy as np

import io

from google.colab import files
uploaded = files.upload()

info_tourism = pd.read_csv(io.BytesIO(uploaded['destinasi.csv']))
info_tourism.head()

from google.colab import files
uploaded = files.upload()

tourism_rating = pd.read_csv(io.BytesIO(uploaded['rating.csv']))
tourism_rating.sample(5)

print(f"Number of places in the dataset: {len(info_tourism['Place_Id'].unique())}")
print(f"Number of users: {len(tourism_rating['User_Id'].unique())}")
print(f"The number of ratings given by the user to the dataset: {len(tourism_rating['User_Id'])}")

"""# Exploratory Data Analysis"""

info_tourism.info()

info_tourism.isnull().sum()

tourism_rating.info()

tourism_rating.isnull().sum()

info_tourism.Category.unique()

"""# Data Preprocessing"""

all_tourism_rate = tourism_rating
all_tourism_rate

all_tourism = pd.merge(all_tourism_rate,info_tourism[["Place_Id","Place_Name","Description","City","Category","Do's 1", "Do's 2", "Dont's 1", "Dont's 2"]],on='Place_Id', how='left')
all_tourism

all_tourism.isnull().sum()

preparation= all_tourism.drop_duplicates("Place_Id")
preparation

place_id = preparation.Place_Id.tolist()

place_name = preparation.Place_Name.tolist()

place_category = preparation.Category.tolist()

place_desc = preparation.Description.tolist()

place_city = preparation.City.tolist()

dos_1 = preparation["Do's 1"].tolist()

dos_2 = preparation["Do's 2"].tolist()

donts_1 = preparation["Dont's 1"].tolist()

donts_2 = preparation["Dont's 2"].tolist()

tourism_new = pd.DataFrame({
    "id":place_id,
    "name":place_name,
    "category":place_category,
    "description":place_desc,
    "city":place_city,
    "do's 1": dos_1,
    "do's 2": dos_2,
    "dont's 1": donts_1,
    "dont's 2": donts_2
})

tourism_new

"""## encode"""

df = tourism_rating
df

user_ids = df.User_Id.unique().tolist()

user_to_user_encoded = {x:i for i, x in enumerate(user_ids)}

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

place_ids = df.Place_Id.unique().tolist()

place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}

place_encoded_to_place = {x: i for x, i in enumerate(place_ids)}

df['user'] = df.User_Id.map(user_to_user_encoded)

df['place'] = df.Place_Id.map(place_to_place_encoded)

num_users = len(user_to_user_encoded)

num_place = len(place_encoded_to_place)

df['Place_Ratings'] = df['Place_Ratings'].values.astype(np.float32)

min_rating = min(df['Place_Ratings'])

max_rating= max(df['Place_Ratings'])

print('Number of User: {}, Number of Place: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_place, min_rating, max_rating
))

"""## Train Test Split"""

df = df.sample(frac=1,random_state=42)
df

x = df[['user','place']].values

y = df['Place_Ratings'].apply(lambda x:(x-min_rating)/(max_rating-min_rating)).values

train_indices = int(0.8 * df.shape[0])

x_train,x_val,y_train,y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices].reshape((-1,1)), #reshape y_train
    y[train_indices:].reshape((-1,1))  #reshape y_val
)

print(x,y)

"""# Training"""

from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_place, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_place = num_place
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-3)
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.place_embedding = layers.Embedding(
            num_place,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-3)
        )
        self.place_bias = layers.Embedding(num_place, 1)
        self.dense_layer = layers.Dense(128, activation='relu')  # Add an additional dense layer
        self.output_layer = layers.Dense(1, activation='sigmoid')  # Add output layer with 1 unit

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        place_vector = self.place_embedding(inputs[:, 1])
        place_bias = self.place_bias(inputs[:, 1])

        dot_user_place = tf.tensordot(user_vector, place_vector, 2)

        x = dot_user_place + user_bias + place_bias
        x = self.dense_layer(x)  # Apply the additional dense layer
        x = self.output_layer(x)  # Apply output layer with 1 unit and sigmoid activation
        return x

model = RecommenderNet(num_users, num_place, 100)

model.compile(
    loss=tf.keras.losses.MeanSquaredError(),
    optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # Update learning rate
    metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val),
)

# Access RMSE and MAE values from the history object
val_rmse = history.history['val_root_mean_squared_error']
val_mae = history.history['val_mean_absolute_error']

# Get the final RMSE and MAE values
final_val_rmse = val_rmse[-1]
final_val_mae = val_mae[-1]

print("Final Validation RMSE:", final_val_rmse)
print("Final Validation MAE:", final_val_mae)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""# Recommendation"""

place_df = tourism_new

user_id = df.User_Id.sample(1).iloc[0]
place_visited_by_user = df[df.User_Id == user_id]

place_not_visited = place_df[~place_df['id'].isin(place_visited_by_user['Place_Id'].values)]['id']
place_not_visited = list(
    set(place_not_visited)
    .intersection(set(place_to_place_encoded.keys()))
)

place_not_visited = [[place_to_place_encoded.get(x)] for x in place_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_place_array = np.hstack(
    ([[user_encoder]] * len(place_not_visited), place_not_visited)
)

place_visited_by_user

ratings = model.predict(user_place_array).flatten()

top_ratings_indices = ratings.argsort()[::-1]
recommended_place_ids = [
    place_encoded_to_place.get(place_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Place with high ratings from user')
print('----' * 8)

top_place_user = (
    place_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)

place_df_rows = place_df[place_df['id'].isin(top_place_user)]
pd.DataFrame(place_df_rows)

print('----' * 8)
print('Top 30 place recommendation')
print('----' * 8)

recommended_place = place_df[place_df['id'].isin(recommended_place_ids)]
recommended_place.head(30)

#slicing category kuliner
recommended_place.loc[recommended_place['category'] == 'kuliner']

#slicing category hotel
recommended_place.loc[recommended_place['category'] == 'Hotel']

#slicing category stay
recommended_place.loc[recommended_place['category'] == 'Stay']

"""# Saving Model"""

import pathlib
#export the model
export_dir = '/tmp/saved_model'
tf.saved_model.save(model, export_dir)

#convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()

#save the model
tflite_model_file = pathlib.Path('/tmp/Tweeze.tflite')
tflite_model_file.write_bytes(tflite_model)

from google.colab import drive
drive.mount('/content/drive')